experiment:
  backend: pytorch
  gpu: 0
  models:
    external.DeepMF:
      meta:
        save_weights: False
        save_recs: False
      #
      lr: [ 1.e-4,  1.e-3,  1.e-2  ]
      # not really relevant
      batch_size: [ 128 ]
      # not really relevant
      reg: [ 1.e-3  ]
      # already 128 gives OOM on rk4 with batch size 256 and no intermediate layers
      # 64 still to try
      embedding_dim: [ 256, 128, 64 ] # [ 256, 128 ]
      #
      user_mlp: [ [] ]
      #
      item_mlp: [ [] ] # [ [ 1024, 512 ], [] ] # [ [1024, 512, 256], [1024, 512], [512, 256] ]
      #
      neg_ratio: [ 5 ] # 1 , 10, 20, 100 ]
      similarity: cosine

      early_stopping:
        verbose: False
        patience: 10
        monitor: nDCG@5