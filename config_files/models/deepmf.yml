experiment:
  backend: pytorch
  models:
    external.DeepMF:
      # still to optimize
      lr: [ 1.e-4 ] #,  1.e-3,  1.e-2]
      #
      batch_size: [ 128 ]
      #
      reg: [ 1.e-3 ]
      # already 128 gives OOM on rk4 with batch size 256 and no intermediate layers
      # 64 still to try
      embedding_dim: [ 128 ] # [ 256, 128 ]
      #
      user_mlp: [ [256] ] # [ [ 512, 256 ], [256], [] ]
      #
      item_mlp: [ [256] ] # [ [ 1024, 512 ], [] ] # [ [1024, 512, 256], [1024, 512], [512, 256] ]
      #
      neg_ratio: 5 # [ 5, 10, 20 ]
      similarity: cosine