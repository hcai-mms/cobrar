experiment:
  gpu: 3
  backend: pytorch
  models:
    external.SiBraR:
      meta:
        save_weights: False
        save_recs: False
      # to optimize
      lr: [ 1.e-7 ]
      batch_size: [ 64 ]
      num_neg: [ 5 ]
      norm_input_feat: [ False, True ]
      input_dim: [ 512 ]
      batch_norm: [ True, False ]
      # Setting to true decreases performance
      norm_sbra_input: [ False ]
      mid_layers: [ [512, 512, 512] ]# [512, 512, 256, 256, 128, 128, 64, 64] ]
      # To optimize
      emb_dim: [ 256 ]
      # When to apply batch normalization
      # This is batch_norm_every in SiBraR
      # 0 ... deactivate, 1+ ... every n layers, -1 ... only last layer
      # WIP still not working
#      b_norm_e: -1
      w_decay: [ 1.e-2 ]
      dropout: [ 1.e-1, 5.e-1, 8.e-1 ]
      # use_user_profile i.e., users interactions for embedding users
      u_prof: [ False ] # False, True ]
      cl_weight: [ 1.e-4 ]
      cl_temp: [ 1.e-1 ]

      early_stopping:
        verbose: False
        patience: 5
        monitor: nDCG@5