experiment:
  gpu: 0
  backend: pytorch
  models:
    external.VBPR:
      lr: [ 1.e-4,  1.e-3,  1.e-2]
      batch_size: [ 128, 256, 512, 1024 ]
      factors: [ 256, 128, 64 ]
      l_w: [  1.e-3,  1.e-2,  1.e-1 ]
      # I experience that 200 is too few on onion
      epochs: 500
      comb_mod: concat