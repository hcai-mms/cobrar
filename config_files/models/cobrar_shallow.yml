experiment:
  gpu: 2
  backend: pytorch
  models:
    external.CoBraR:
      #
      lr: [ 1.e-5 ] # Way better than 1.e-3 and 1.e-2
      # not really relevant
      batch_size: [ 128 ]
      # not really relevant
      reg: [ 1.e-1 ]
      # already 128 gives OOM on rk4 with batch size 256 and no intermediate layers
      # 64 still to try
      embedding_dim: [ 64 ] # [ 256, 128, 64 ]: results were all over the place
      #
      user_mlp: [ [] ]
      #
      item_mlp: [ [] ] # [ [ 1024, 512 ], [] ] # [ [1024, 512, 256], [1024, 512], [512, 256] ]
      #
      collaborative_branch: [ [ 2048 ], [ 1024 ], [ 512 ], [ 256 ]] # [ [1024, 512, 256], [1024, 512], [512, 256] ]
      #
      neg_ratio: [ 5 ] # 1 , 10, 20, 100 ]
      similarity: cosine

      early_stopping:
        verbose: False
        patience: 10
        monitor: nDCG@5